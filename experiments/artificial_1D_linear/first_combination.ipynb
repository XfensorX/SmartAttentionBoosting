{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data import artificial_1D_linear as data\n",
    "from experiments.artificial_1D_linear.documentation import (\n",
    "    plot_data_split,\n",
    "    plot_predictions,\n",
    ")\n",
    "from models.DenseNetwork import DenseNetwork\n",
    "from models.multi_output_net import MultiOutputNet\n",
    "from utils.federated_learning import average_models\n",
    "from utils.self_learning import combine_two\n",
    "\n",
    "torch.manual_seed(42)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logging_dir(name: str):\n",
    "    return f\"../../logs/artificial_1D_linear/{name}/{time.strftime('%m-%d-%H-%M-%S', time.localtime())}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating different learning algorithms on artifical 1D Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Hyperparameters are set in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "INPUT_FEATURES = 1\n",
    "OUTPUT_FEATURES = 1\n",
    "ARCHITECTURE = [12, 12, 12, 12]\n",
    "\n",
    "LOSS_FN = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data gets split into a 20/80 test/training split randomly. The data is visualized in the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*data.get_data(\"train\"), label=\"Training Data\")\n",
    "plt.plot(*data.get_data(\"test\"), label=\"Test Data\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    test_dataloader = data.get_dataloader(\"test\")\n",
    "    assert 1 == len(test_dataloader)\n",
    "\n",
    "    for x, y in test_dataloader:\n",
    "        return LOSS_FN(model(x), y).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Standard Batch-Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.get_dataloader(\"train\", BATCH_SIZE, shuffle=True)\n",
    "\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_network = DenseNetwork(\n",
    "    DenseNetwork.Config(INPUT_FEATURES, ARCHITECTURE, OUTPUT_FEATURES)\n",
    ")\n",
    "model_name = \"standard_bgd\"\n",
    "optimizer = torch.optim.Adam(dense_network.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(get_logging_dir(model_name, \"artificial_1D_linear\"))\n",
    "\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    dense_network.train()\n",
    "    losses = []\n",
    "    for x, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = dense_network(x)\n",
    "        loss = LOSS_FN(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    writer.add_scalar(\"loss\", sum(losses) / len(losses), e)\n",
    "\n",
    "    writer.add_scalar(\"test_loss\", evaluate(dense_network), e)\n",
    "\n",
    "\n",
    "plot_predictions(dense_network, model_name, writer)\n",
    "\n",
    "writer.add_hparams(\n",
    "    {\n",
    "        \"epochs\": EPOCHS,\n",
    "    },\n",
    "    {\n",
    "        \"MSE Test\": evaluate(dense_network),\n",
    "    },\n",
    "    run_name=\".\",\n",
    ")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. FedAvg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to solve the problem using FedAvg and 2 clients, to compare it to the novel method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 10\n",
    "CLIENT_IDs = range(NUM_CLIENTS)\n",
    "\n",
    "SPLIT_TYPE = \"interval\"\n",
    "\n",
    "COMMUNICATION_ROUNDS = 100\n",
    "CLIENT_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"FedAvg_{NUM_CLIENTS}clients_{SPLIT_TYPE}-split\"\n",
    "writer = SummaryWriter(get_logging_dir(model_name, \"artificial_1D_linear\"))\n",
    "\n",
    "\n",
    "client_train_dataloaders = data.get_client_train_dataloaders(\n",
    "    NUM_CLIENTS, SPLIT_TYPE, BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "plot_data_split(client_train_dataloaders, writer)\n",
    "\n",
    "\n",
    "clients = [\n",
    "    DenseNetwork(DenseNetwork.Config(INPUT_FEATURES, ARCHITECTURE, OUTPUT_FEATURES))\n",
    "    for _ in CLIENT_IDs\n",
    "]\n",
    "\n",
    "client_optimizers = [\n",
    "    torch.optim.Adam(clients[client_no].parameters(), lr=0.001)\n",
    "    for client_no in CLIENT_IDs\n",
    "]\n",
    "\n",
    "\n",
    "for cr in range(COMMUNICATION_ROUNDS):\n",
    "    for client_no, client in zip(CLIENT_IDs, clients):\n",
    "        client.train()\n",
    "\n",
    "        for ce in range(CLIENT_EPOCHS):\n",
    "            losses = []\n",
    "            for x, y in client_train_dataloaders[client_no]:\n",
    "                client_optimizers[client_no].zero_grad()\n",
    "                y_hat = client(x)\n",
    "                loss = LOSS_FN(y_hat, y)\n",
    "                loss.backward()\n",
    "                client_optimizers[client_no].step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            writer.add_scalar(\n",
    "                f\"loss/client{client_no}\",\n",
    "                sum(losses) / len(losses),\n",
    "                cr * CLIENT_EPOCHS + ce,\n",
    "            )\n",
    "\n",
    "    for client, client_no in zip(clients, CLIENT_IDs):\n",
    "        writer.add_scalar(f\"test_loss/client{client_no}\", evaluate(client), cr)\n",
    "\n",
    "    global_model = average_models(clients)\n",
    "    writer.add_scalar(\"test_loss\", evaluate(global_model), cr * CLIENT_EPOCHS)\n",
    "\n",
    "    for client in clients:\n",
    "        client.load_state_dict(global_model.state_dict())\n",
    "\n",
    "\n",
    "plot_predictions(global_model, model_name, writer)\n",
    "writer.add_hparams(\n",
    "    {\n",
    "        \"client_epochs\": CLIENT_EPOCHS,\n",
    "        \"num_clients\": NUM_CLIENTS,\n",
    "        \"communication_rounds\": COMMUNICATION_ROUNDS,\n",
    "        \"split_type\": SPLIT_TYPE,\n",
    "    },\n",
    "    {\n",
    "        \"MSE Test\": evaluate(global_model),\n",
    "    },\n",
    "    run_name=\".\",\n",
    ")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 2\n",
    "CLIENT_IDs = range(NUM_CLIENTS)\n",
    "\n",
    "COMMUNICATION_ROUNDS = 100\n",
    "CLIENT_EPOCHS = 100\n",
    "\n",
    "SPLIT_TYPE = \"random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"MyFed_{NUM_CLIENTS}clients_{SPLIT_TYPE}-split\"\n",
    "writer = SummaryWriter(get_logging_dir(model_name, \"artificial_1D_linear\"))\n",
    "\n",
    "clients = [\n",
    "    MultiOutputNet(ARCHITECTURE, INPUT_FEATURES, OUTPUT_FEATURES) for _ in CLIENT_IDs\n",
    "]\n",
    "\n",
    "client_train_dataloaders = data.get_client_train_dataloaders(\n",
    "    NUM_CLIENTS, SPLIT_TYPE, BATCH_SIZE, shuffle=True\n",
    ")\n",
    "plot_data_split(client_train_dataloaders, writer)\n",
    "\n",
    "client_optimizers: list[None | torch.optim.Optimizer] = [None for _ in CLIENT_IDs]\n",
    "\n",
    "\n",
    "for cr in range(COMMUNICATION_ROUNDS):\n",
    "    # train each client individually\n",
    "    for client_no, client in zip(CLIENT_IDs, clients):\n",
    "        client.train()\n",
    "\n",
    "        client_optimizers[client_no] = torch.optim.Adam(\n",
    "            clients[client_no].parameters(), lr=0.001\n",
    "        )\n",
    "\n",
    "        # train each client for certain epochs\n",
    "        for ce in range(CLIENT_EPOCHS):\n",
    "            losses = []\n",
    "            for x, y in client_train_dataloaders[client_no]:\n",
    "                client_optimizers[client_no].zero_grad()\n",
    "                y_hat = client(x)\n",
    "                loss = LOSS_FN(y_hat, y)\n",
    "                loss.backward()\n",
    "                client_optimizers[client_no].step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            writer.add_scalar(\n",
    "                f\"loss/client{client_no}\",\n",
    "                sum(losses) / len(losses),\n",
    "                cr * CLIENT_EPOCHS + ce,\n",
    "            )\n",
    "\n",
    "    for client, client_no in zip(clients, CLIENT_IDs):\n",
    "        writer.add_scalar(f\"test_loss/client{client_no}\", evaluate(client), cr)\n",
    "\n",
    "    if cr == COMMUNICATION_ROUNDS - 1:\n",
    "        global_model = average_models(clients)\n",
    "    else:\n",
    "        global_model = combine_two(\n",
    "            clients[0],\n",
    "            clients[1],\n",
    "            similarity_threshold_in_degree=10,\n",
    "            new_weight_initialization=\"noise\",\n",
    "        )\n",
    "\n",
    "    writer.add_scalar(\"test_loss\", evaluate(global_model), cr * CLIENT_EPOCHS)\n",
    "\n",
    "    #### ----- Unimportant ----- TODO: remove\n",
    "    from IPython.display import clear_output\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Communication Round {cr}\\{  COMMUNICATION_ROUNDS }\")\n",
    "    print(\"\\r global model: \", global_model.layers, end=\"\")\n",
    "    #### //// ----- Unimportant -----\n",
    "\n",
    "\n",
    "    clients = [copy.deepcopy(global_model) for _ in CLIENT_IDs]\n",
    "\n",
    "    if cr == COMMUNICATION_ROUNDS - 2:\n",
    "        for client in clients:\n",
    "            client.freeze_all()\n",
    "            client.unfreeze_layer(-1)\n",
    "            client.unfreeze_output_scaling()\n",
    "\n",
    "plot_predictions(global_model, model_name, writer)\n",
    "writer.add_hparams(\n",
    "    {\n",
    "        \"client_epochs\": CLIENT_EPOCHS,\n",
    "        \"num_clients\": NUM_CLIENTS,\n",
    "        \"communication_rounds\": COMMUNICATION_ROUNDS,\n",
    "        \"split_type\": SPLIT_TYPE,\n",
    "    },\n",
    "    {\n",
    "        \"MSE Test\": evaluate(global_model),\n",
    "    },\n",
    "    run_name=\".\",\n",
    ")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
